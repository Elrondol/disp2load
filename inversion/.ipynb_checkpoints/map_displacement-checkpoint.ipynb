{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b97125-fdf5-428e-b5e2-7b4459545c68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import gzip\n",
    "import fastparquet\n",
    "import os \n",
    "from datetime import datetime, timedelta \n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5eb111-0c37-4bc3-9306-efe60faa32c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CAM6' 'MJPK' 'P471' 'SACY' 'SIO5' 'TRAK' 'VTIS' 'LBC1' 'PVHS' 'MHMS'\n",
      " 'BKMS' 'FVPK' 'CACL' 'P474' 'LBNO' 'P472' 'P475' 'CACJ' 'RAAP' 'CSDH'\n",
      " 'P470' 'HOL3' 'RSTP' 'P583']\n",
      "0.57484 0.562835 0.928909 0.547306 0.223273 0.592646\n"
     ]
    }
   ],
   "source": [
    "#on commence par lire la liste de sstations, on utilise \n",
    "#alors cette liste de stations pour déterminer les datasets à lire et ainsi on lit les div\n",
    "data_directory = '/media/parisnic/STOCKAGE/M2/GNSS_US/'\n",
    "\n",
    "meta = pd.read_csv('stations.csv')\n",
    "stations = meta['name'].values\n",
    "lats = meta['lat'].values\n",
    "lons = meta['lon'].values\n",
    "heights = meta['height'].values\n",
    "print(stations)\n",
    "\n",
    "#maintenant on va chercher les fichiers pour \n",
    "year = 2023\n",
    "dataframes_dict = {}\n",
    "\n",
    "# Loop through the file names and load dataframes into the dictionary\n",
    "for sta in stations:\n",
    "    # Load the dataframe from the parquet file\n",
    "    dataframe = pd.read_parquet(f'{data_directory}{sta}/data_{year}.parquet')\n",
    "    dataframes_dict[sta] = dataframe\n",
    "\n",
    "#on cherche la valeur de vmax (et vmin comme étant - vmax  ) pour tous les types de données pour l'ensemble des stations et pour n'importe quel temps -> permet après\n",
    "#d'avoir un cmap fixe au cours du temps\n",
    "\n",
    "vmax_vref = 0\n",
    "vmax_nref = 0\n",
    "vmax_eref = 0\n",
    "vmax_vmean = 0\n",
    "vmax_nmean = 0\n",
    "vmax_emean = 0\n",
    "\n",
    "for i,sta in enumerate(stations):\n",
    "    temp_vref = np.max(np.abs(dataframes_dict[sta]['v-ref(m)'].values))\n",
    "    temp_nref = np.max(np.abs(dataframes_dict[sta]['n-ref(m)'].values))\n",
    "    temp_eref = np.max(np.abs(dataframes_dict[sta]['e-ref(m)'].values))\n",
    "    temp_vmean = np.max(np.abs(dataframes_dict[sta]['v-mean(m)'].values))\n",
    "    temp_nmean = np.max(np.abs(dataframes_dict[sta]['n-mean(m)'].values))\n",
    "    temp_emean = np.max(np.abs(dataframes_dict[sta]['e-mean(m)'].values))\n",
    "    \n",
    "    if temp_vref>vmax_vref and temp_vref<1:\n",
    "        vmax_vref=temp_vref\n",
    "    if temp_nref>vmax_nref and temp_nref<1:\n",
    "        vmax_nref=temp_nref\n",
    "    if temp_eref>vmax_eref and temp_eref<1:\n",
    "        vmax_eref=temp_eref\n",
    "    if temp_vmean>vmax_vmean and temp_vmean<1:\n",
    "        vmax_vmean=temp_vmean\n",
    "    if temp_nmean>vmax_nmean and temp_nmean<1:\n",
    "        vmax_nmean=temp_nmean\n",
    "    if temp_emean>vmax_emean and temp_emean<1:\n",
    "        vmax_emean=temp_emean\n",
    "\n",
    "print(vmax_vref,vmax_nref,vmax_eref,vmax_vmean,vmax_nmean,vmax_emean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4763c126-0482-4dd5-9a7a-d927c257f432",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "289it [05:30,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "#maintenant qu'on a chargé les dataframes, on va pouvoir itérer sur les dataframes au sein d'un itération sur le temps\n",
    "#on créé à présent l'array de temps sur lequel on va chercher les valeurs aux différentes stations\n",
    "\n",
    "path2export = '/media/parisnic/STOCKAGE/M2/maps/'\n",
    "\n",
    "tbeg = datetime(2023,8,27)\n",
    "tend = datetime(2023,8,28)\n",
    "times = [tbeg]\n",
    "while times[-1]!=tend:\n",
    "    times.append(times[-1]+timedelta(minutes=5))\n",
    "\n",
    "    \n",
    "for i,t in tqdm(enumerate(times)):\n",
    "    n_refs  = np.zeros(len(stations))\n",
    "    e_refs  = np.zeros(len(stations))\n",
    "    v_refs  = np.zeros(len(stations))\n",
    "    \n",
    "    n_means  = np.zeros(len(stations))\n",
    "    e_means  = np.zeros(len(stations))\n",
    "    v_means  = np.zeros(len(stations))\n",
    "    for j, sta in enumerate(stations):\n",
    "        try:\n",
    "            idx = np.where(dataframes_dict[sta]['Time_UTC']==t)[0][0] #trouve l'indice de dans le tableau correspondant au bon temps -> on va alors chercher les params à cetindice pour cette station\n",
    "            n_refs[j] = dataframes_dict[sta]['n-ref(m)'].values[idx]\n",
    "            e_refs[j] = dataframes_dict[sta]['e-ref(m)'].values[idx]\n",
    "            v_refs[j] = dataframes_dict[sta]['v-ref(m)'].values[idx]\n",
    "            n_means[j] = dataframes_dict[sta]['n-mean(m)'].values[idx]\n",
    "            e_means[j] = dataframes_dict[sta]['e-mean(m)'].values[idx]\n",
    "            v_means[j] = dataframes_dict[sta]['v-mean(m)'].values[idx]\n",
    "        except:\n",
    "            n_refs[j] = np.nan\n",
    "            e_refs[j] = np.nan\n",
    "            v_refs[j] = np.nan\n",
    "            n_means[j] = np.nan\n",
    "            e_means[j] = np.nan\n",
    "            v_means[j] = np.nan\n",
    "    \n",
    "    #maintenant qu'on a les valeurs des différentes station au temps considéré, il nous faut à présent plotter la carte, on commence par la faire en 2D sans altitute\n",
    "    fig, ax = plt.subplots(2,3, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.5)\n",
    "    im0 = ax[0,0].scatter(lons,lats,c=v_refs,cmap='bwr',vmax=vmax_vref,vmin=-vmax_vref); ax[0,0].set_title('v-ref (m)');plt.colorbar(im0, ax=ax[0, 0])\n",
    "    im1 = ax[0,1].scatter(lons,lats,c=n_refs,cmap='bwr',vmax=vmax_nref,vmin=-vmax_nref); ax[0,1].set_title('n-ref (m)');plt.colorbar(im1, ax=ax[0, 1])\n",
    "    im2 = ax[0,2].scatter(lons,lats,c=e_refs,cmap='bwr',vmax=vmax_eref,vmin=-vmax_eref); ax[0,2].set_title('e-ref (m)');plt.colorbar(im2, ax=ax[0, 2])\n",
    "    im3 = ax[1,0].scatter(lons,lats,c=v_means,cmap='bwr',vmax=vmax_vmean,vmin=-vmax_vmean); ax[1,0].set_title('v-mean (m)');plt.colorbar(im3, ax=ax[1, 0])\n",
    "    im4 = ax[1,1].scatter(lons,lats,c=n_means,cmap='bwr',vmax=vmax_nmean,vmin=-vmax_nmean); ax[1,1].set_title('n-mean (m)');plt.colorbar(im4, ax=ax[1, 1])\n",
    "    im5 = ax[1,2].scatter(lons,lats,c=e_means,cmap='bwr',vmax=vmax_emean,vmin=-vmax_emean); ax[1,2].set_title('e-mean (m)');plt.colorbar(im5, ax=ax[1, 2])\n",
    "    plt.savefig(f'{path2export}{t}.png',bbox_inches='tight')\n",
    "    \n",
    "    plt.close('all')\n",
    "    \n",
    "    \n",
    "# print(dataframes_dict['CAM6']['Time_UTC'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17b26d2-0f39-46ca-95cd-a1b6bac29ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f9f3f-b5df-49d6-896a-78a68e4c4dc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d0c6e6-57f0-4fba-9e21-4717508f6fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
